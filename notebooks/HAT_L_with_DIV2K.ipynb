{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer\n"
          ]
        }
      ],
      "source": [
        "%cd /Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17p7EO3-WUaJ"
      },
      "source": [
        "# Preperation of the Environment & Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUDSkDmLYBjD"
      },
      "source": [
        "**Dowload the original HAT codebase.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j5Jkg3AKWDV9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/XPixelGroup/HAT.git .codebases/HAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ixd68KYJyj"
      },
      "source": [
        "**Install dependencies.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a61va9wJYFSF"
      },
      "outputs": [],
      "source": [
        "%pip install torch torchvision torchaudio\n",
        "%pip install -r .codebases/HAT/requirements.txt\n",
        "!python .codebases/HAT/setup.py develop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAleu695Y8ws"
      },
      "source": [
        "**Download dataset DIV2K and unzip it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCQ8zcw0ZAM4"
      },
      "outputs": [],
      "source": [
        "!wget -c http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip -P .datasets/DIV2K\n",
        "!unzip .datasets/DIV2K/DIV2K_train_HR.zip -d .datasets/DIV2K\n",
        "!wget -c http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip -P .datasets/DIV2K\n",
        "!unzip .datasets/DIV2K/DIV2K_valid_HR.zip -d .datasets/DIV2K\n",
        "!wget -c http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X2.zip -P .datasets/DIV2K\n",
        "!unzip .datasets/DIV2K/DIV2K_train_LR_bicubic_X2.zip -d .datasets/DIV2K\n",
        "!wget -c http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X2.zip -P .datasets/DIV2K\n",
        "!unzip .datasets/DIV2K/DIV2K_valid_LR_bicubic_X2.zip -d .datasets/DIV2K\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Rename the directories.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mv .datasets/DIV2K/DIV2K_train_HR .datasets/DIV2K/train_hr\n",
        "!mv .datasets/DIV2K/DIV2K_valid_HR .datasets/DIV2K/valid_hr\n",
        "!mv .datasets/DIV2K/DIV2K_train_LR_bicubic/X2 .datasets/DIV2K/train_lr\n",
        "!mv .datasets/DIV2K/DIV2K_valid_LR_bicubic/X2 .datasets/DIV2K/valid_lr\n",
        "!rm -rf .datasets/DIV2K/DIV2K_train_LR_bicubic\n",
        "!rm -rf .datasets/DIV2K/DIV2K_valid_LR_bicubic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvJtcbjsa16f"
      },
      "source": [
        "**Get the mutliscale converter & sub-image maker scripts from Real-ESRGAN codebase.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aiZOsU9bAgZ"
      },
      "outputs": [],
      "source": [
        "!mkdir .scripts\n",
        "!wget https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/scripts/generate_multiscale_DF2K.py -P .scripts\n",
        "!wget https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/scripts/extract_subimages.py -P .scripts\n",
        "!wget https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/scripts/generate_meta_info.py -P .scripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrDKmBSzbbI2"
      },
      "source": [
        "**Generate multiscale images.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7Dgzbpybi48"
      },
      "outputs": [],
      "source": [
        "!python .scripts/generate_multiscale_DF2K.py --input .datasets/DIV2K/train_hr --output .datasets/DIV2K/train_hr_multiscale\n",
        "!python .scripts/generate_multiscale_DF2K.py --input .datasets/DIV2K/valid_hr --output .datasets/DIV2K/valid_hr_multiscale\n",
        "!python .scripts/generate_multiscale_DF2K.py --input .datasets/DIV2K/train_lr --output .datasets/DIV2K/train_lr_multiscale\n",
        "!python .scripts/generate_multiscale_DF2K.py --input .datasets/DIV2K/valid_lr --output .datasets/DIV2K/valid_lr_multiscale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwAl2CYPgo8C"
      },
      "source": [
        "**Generate sub-images.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU1FELDjgoUs"
      },
      "outputs": [],
      "source": [
        "!python .scripts/extract_subimages.py --input .datasets/DIV2K/train_hr --output .datasets/DIV2K/sub_train_hr --crop_size 400 --step 200\n",
        "!python .scripts/extract_subimages.py --input .datasets/DIV2K/valid_hr --output .datasets/DIV2K/sub_valid_hr --crop_size 400 --step 200\n",
        "!python .scripts/extract_subimages.py --input .datasets/DIV2K/train_lr --output .datasets/DIV2K/sub_train_lr --crop_size 200 --step 100\n",
        "!python .scripts/extract_subimages.py --input .datasets/DIV2K/valid_lr --output .datasets/DIV2K/sub_valid_lr --crop_size 200 --step 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the original, and mutliscale images.\n",
        "!rm -r .datasets/DIV2K/train_hr\n",
        "!rm -r .datasets/DIV2K/valid_hr\n",
        "!rm -r .datasets/DIV2K/train_lr\n",
        "!rm -r .datasets/DIV2K/valid_lr\n",
        "!rm -r .datasets/DIV2K/train_hr_multiscale\n",
        "!rm -r .datasets/DIV2K/valid_hr_multiscale\n",
        "!rm -r .datasets/DIV2K/train_lr_multiscale\n",
        "!rm -r .datasets/DIV2K/valid_lr_multiscale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Check the size of the dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<OK>] Number of images in sub_train_hr and sub_train_lr are equal.\n",
            "[<OK>] Number of images in sub_valid_hr and sub_valid_lr are equal.\n",
            "\n",
            "Total Disk Usage\n",
            " 22G\t.datasets/DIV2K/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check the sizes of the datasets' image count.\n",
        "EQUALITY_CHECK = {\n",
        "    \"sub_train_hr\": \"sub_train_lr\",\n",
        "    \"sub_valid_hr\": \"sub_valid_lr\",\n",
        "}\n",
        "\n",
        "for hr, lr in EQUALITY_CHECK.items():\n",
        "    hr_path = os.path.join(\".datasets/DIV2K\", hr)\n",
        "    lr_path = os.path.join(\".datasets/DIV2K\", lr)\n",
        "    hr_images = os.listdir(hr_path)\n",
        "    lr_images = os.listdir(lr_path)\n",
        "    if len(hr_images) != len(lr_images):\n",
        "        print(f\"[ERROR] Number of images in {hr} and {lr} are not equal.\")\n",
        "    print(f\"[<OK>] Number of images in {hr} and {lr} are equal.\")\n",
        "\n",
        "\n",
        "# Check the total disk usage.\n",
        "!echo \"\\nTotal Disk Usage\"\n",
        "!du -sh .datasets/DIV2K/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Select a Subset of Whole Dataset for Faster Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "from os.path import join, relpath\n",
        "from cv2 import imread\n",
        "from numpy.random import rand\n",
        "\n",
        "# Change these variables to your own dataset location.\n",
        "METADATAS = [\n",
        "    {\n",
        "        'root_location': '.datasets/DIV2K/',\n",
        "        'dataset_location': '.datasets/DIV2K/sub_train_hr/',\n",
        "        'meta_data_location': '.datasets/DIV2K/meta_info_train.txt'\n",
        "    },\n",
        "    {\n",
        "        'root_location': '.datasets/DIV2K/',\n",
        "        'dataset_location': '.datasets/DIV2K/sub_valid_hr/',\n",
        "        'meta_data_location': '.datasets/DIV2K/meta_info_valid.txt'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Select 20% of each dataset for faster training, and place them into metadata.\n",
        "for meta_data in METADATAS:\n",
        "    with open(meta_data['meta_data_location'], 'w') as meta_info_file:\n",
        "        # Get all image paths.\n",
        "        all_files = sorted(glob(join(meta_data['dataset_location'], '*')))\n",
        "        \n",
        "        # Traverse all images.\n",
        "        for image_path in all_files:\n",
        "\n",
        "            # Select a random number, if it is greater than 0.2, skip this image.\n",
        "            if rand() > 0.2:\n",
        "                continue\n",
        "\n",
        "            # Check the image.\n",
        "            try:\n",
        "                image = imread(image_path)\n",
        "            except (IOError, OSError) as error:\n",
        "                print(f'Read {image_path} error: {error}')\n",
        "                continue\n",
        "            if image is None:\n",
        "                print(f'Img is None: {image_path}')\n",
        "                continue\n",
        "\n",
        "            # Get the relative path.\n",
        "            img_name = relpath(image_path, meta_data['root_location'])\n",
        "            \n",
        "            # Write into metadata.\n",
        "            meta_info_file.write(f'{img_name}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Configuration File for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Copy a new configuration file from the original one.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p .options/train\n",
        "!cp .codebases/HAT/options/train/train_HAT-S_SRx2_from_scratch.yml .options/train/hat_2x_div2k.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the configuration YAML file and change the parameters needed.\n",
        "import yaml\n",
        "\n",
        "with open('.options/train/hat_2x_div2k.yml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "if config['datasets']['train']['name'] != \"DIV2K_Training\":\n",
        "    config['datasets']['train']['name'] = \"DIV2K_Training\"\n",
        "    config['datasets']['train']['dataroot_gt'] = \".datasets/DIV2K/\"\n",
        "    config['datasets']['train']['dataroot_lq'] = \".datasets/DIV2K/sub_train_lr\" # It needs sub-dir as well.\n",
        "    config['datasets']['train']['meta_info_file'] = \".datasets/DIV2K/meta_info_train.txt\"\n",
        "    config['datasets']['val']['name'] = \"DIV2K_Validation\"\n",
        "    config['datasets']['val']['dataroot_gt'] = \".datasets/DIV2K/\"\n",
        "    config['datasets']['val']['dataroot_lq'] = \".datasets/DIV2K/sub_valid_lr\"  # It needs sub-dir as well.\n",
        "    del config['datasets']['val_2']\n",
        "    del config['datasets']['val_3']\n",
        "\n",
        "    # Save the configuration YAML file.\n",
        "    with open('.options/train/hat_2x_div2k_test.yml', 'w') as file:\n",
        "        yaml.dump(config, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demo Training (Short Period & w/ Debugging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "Disable distributed.\n",
            "Path already exists. Rename it to /Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/.codebases/HAT/experiments/debug_train_HAT-S_SRx2_from_scratch_archived_20231211_004307\n",
            "2023-12-11 00:43:07,394 INFO: \n",
            "                ____                _       _____  ____\n",
            "               / __ ) ____ _ _____ (_)_____/ ___/ / __ \\\n",
            "              / __  |/ __ `// ___// // ___/\\__ \\ / /_/ /\n",
            "             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/\n",
            "            /_____/ \\__,_//____//_/ \\___//____//_/ |_|\n",
            "     ______                   __   __                 __      __\n",
            "    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /\n",
            "   / / __ / __ \\ / __ \\ / __  /  / /   / / / // ___// //_/  / /\n",
            "  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/\n",
            "  \\____/ \\____/ \\____/ \\____/  /_____/\\____/ \\___//_/|_|  (_)\n",
            "    \n",
            "Version Information: \n",
            "\tBasicSR: 1.3.4.9\n",
            "\tPyTorch: 2.1.1\n",
            "\tTorchVision: 0.16.1\n",
            "2023-12-11 00:43:07,394 INFO: \n",
            "  name: debug_train_HAT-S_SRx2_from_scratch\n",
            "  model_type: HATModel\n",
            "  scale: 2\n",
            "  num_gpu: 0\n",
            "  manual_seed: 0\n",
            "  datasets:[\n",
            "    train:[\n",
            "      name: DIV2K_Training\n",
            "      type: PairedImageDataset\n",
            "      dataroot_gt: .datasets/DIV2K/\n",
            "      dataroot_lq: .datasets/DIV2K/sub_train_lr/\n",
            "      meta_info_file: .datasets/DIV2K/meta_info_train.txt\n",
            "      io_backend:[\n",
            "        type: disk\n",
            "      ]\n",
            "      gt_size: 128\n",
            "      use_hflip: True\n",
            "      use_rot: True\n",
            "      use_shuffle: True\n",
            "      num_worker_per_gpu: 6\n",
            "      batch_size_per_gpu: 4\n",
            "      dataset_enlarge_ratio: 1\n",
            "      prefetch_mode: None\n",
            "      phase: train\n",
            "      scale: 2\n",
            "    ]\n",
            "    val:[\n",
            "      name: DIV2K_Validation\n",
            "      type: PairedImageDataset\n",
            "      dataroot_gt: .datasets/DIV2K/\n",
            "      dataroot_lq: .datasets/DIV2K/sub_valid_lr/\n",
            "      meta_info_file: .datasets/DIV2K/meta_info_valid.txt\n",
            "      io_backend:[\n",
            "        type: disk\n",
            "      ]\n",
            "      phase: val\n",
            "      scale: 2\n",
            "    ]\n",
            "  ]\n",
            "  network_g:[\n",
            "    type: HAT\n",
            "    upscale: 2\n",
            "    in_chans: 3\n",
            "    img_size: 64\n",
            "    window_size: 16\n",
            "    compress_ratio: 24\n",
            "    squeeze_factor: 24\n",
            "    conv_scale: 0.01\n",
            "    overlap_ratio: 0.5\n",
            "    img_range: 1.0\n",
            "    depths: [6, 6, 6, 6, 6, 6]\n",
            "    embed_dim: 144\n",
            "    num_heads: [6, 6, 6, 6, 6, 6]\n",
            "    mlp_ratio: 2\n",
            "    upsampler: pixelshuffle\n",
            "    resi_connection: 1conv\n",
            "  ]\n",
            "  path:[\n",
            "    pretrain_network_g: None\n",
            "    strict_load_g: True\n",
            "    resume_state: None\n",
            "    experiments_root: /Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/.codebases/HAT/experiments/debug_train_HAT-S_SRx2_from_scratch\n",
            "    models: /Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/.codebases/HAT/experiments/debug_train_HAT-S_SRx2_from_scratch/models\n",
            "    training_states: /Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/.codebases/HAT/experiments/debug_train_HAT-S_SRx2_from_scratch/training_states\n",
            "    log: /Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/.codebases/HAT/experiments/debug_train_HAT-S_SRx2_from_scratch\n",
            "    visualization: /Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/.codebases/HAT/experiments/debug_train_HAT-S_SRx2_from_scratch/visualization\n",
            "  ]\n",
            "  train:[\n",
            "    ema_decay: 0.999\n",
            "    optim_g:[\n",
            "      type: Adam\n",
            "      lr: 0.0002\n",
            "      weight_decay: 0\n",
            "      betas: [0.9, 0.99]\n",
            "    ]\n",
            "    scheduler:[\n",
            "      type: MultiStepLR\n",
            "      milestones: [250000, 400000, 450000, 475000]\n",
            "      gamma: 0.5\n",
            "    ]\n",
            "    total_iter: 500000\n",
            "    warmup_iter: -1\n",
            "    pixel_opt:[\n",
            "      type: L1Loss\n",
            "      loss_weight: 1.0\n",
            "      reduction: mean\n",
            "    ]\n",
            "  ]\n",
            "  val:[\n",
            "    val_freq: 8\n",
            "    save_img: False\n",
            "    pbar: False\n",
            "    metrics:[\n",
            "      psnr:[\n",
            "        type: calculate_psnr\n",
            "        crop_border: 2\n",
            "        test_y_channel: True\n",
            "        better: higher\n",
            "      ]\n",
            "      ssim:[\n",
            "        type: calculate_ssim\n",
            "        crop_border: 2\n",
            "        test_y_channel: True\n",
            "        better: higher\n",
            "      ]\n",
            "    ]\n",
            "  ]\n",
            "  logger:[\n",
            "    print_freq: 1\n",
            "    save_checkpoint_freq: 8\n",
            "    use_tb_logger: True\n",
            "    wandb:[\n",
            "      project: None\n",
            "      resume_id: None\n",
            "    ]\n",
            "  ]\n",
            "  dist_params:[\n",
            "    backend: nccl\n",
            "    port: 29500\n",
            "  ]\n",
            "  dist: False\n",
            "  rank: 0\n",
            "  world_size: 1\n",
            "  auto_resume: False\n",
            "  is_train: True\n",
            "  root_path: /Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/.codebases/HAT\n",
            "\n",
            "2023-12-11 00:43:07,416 INFO: Dataset [PairedImageDataset] - DIV2K_Training is built.\n",
            "2023-12-11 00:43:07,416 INFO: Training statistics:\n",
            "\tNumber of train images: 9904\n",
            "\tDataset enlarge ratio: 1\n",
            "\tBatch size per gpu: 4\n",
            "\tWorld size (gpu number): 1\n",
            "\tRequire iter number per epoch: 2476\n",
            "\tTotal epochs: 202; iters: 500000.\n",
            "2023-12-11 00:43:07,418 INFO: Dataset [PairedImageDataset] - DIV2K_Validation is built.\n",
            "2023-12-11 00:43:07,419 INFO: Number of val images/folders in DIV2K_Validation: 1202\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "2023-12-11 00:43:07,609 INFO: Network [HAT] is created.\n",
            "2023-12-11 00:43:07,617 INFO: Network: HAT, with parameters: 9,473,471\n",
            "2023-12-11 00:43:07,617 INFO: HAT(\n",
            "  (conv_first): Conv2d(3, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (patch_unembed): PatchUnEmbed()\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (layers): ModuleList(\n",
            "    (0): RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0): HAB(\n",
            "            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(144, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(6, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1-5): 5 x HAB(\n",
            "            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(144, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(6, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "    (1-5): 5 x RHAG(\n",
            "      (residual_group): AttenBlocks(\n",
            "        (blocks): ModuleList(\n",
            "          (0-5): 6 x HAB(\n",
            "            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (conv_block): CAB(\n",
            "              (cab): Sequential(\n",
            "                (0): Conv2d(144, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (1): GELU(approximate='none')\n",
            "                (2): Conv2d(6, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "                (3): ChannelAttention(\n",
            "                  (attention): Sequential(\n",
            "                    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "                    (1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (2): ReLU(inplace=True)\n",
            "                    (3): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "                    (4): Sigmoid()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (overlap_attn): OCAB(\n",
            "          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (patch_embed): PatchEmbed()\n",
            "      (patch_unembed): PatchUnEmbed()\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "  (conv_after_body): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_before_upsample): Sequential(\n",
            "    (0): Conv2d(144, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "  )\n",
            "  (upsample): Upsample(\n",
            "    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): PixelShuffle(upscale_factor=2)\n",
            "  )\n",
            "  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "2023-12-11 00:43:07,620 INFO: Use Exponential Moving Average with decay: 0.999\n",
            "2023-12-11 00:43:07,803 INFO: Network [HAT] is created.\n",
            "2023-12-11 00:43:07,821 INFO: Loss [L1Loss] is created.\n",
            "2023-12-11 00:43:07,823 INFO: Model [HATModel] is created.\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "2023-12-11 00:43:17,913 INFO: Start training from epoch: 0, iter: 0\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/div2k_venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "2023-12-11 00:43:55,586 INFO: [debug..][epoch:  0, iter:       1, lr:(2.000e-04,)] [eta: 0:00:22, time (data): 37.672 (12.685)] l_pix: 2.4199e-01 \n",
            "2023-12-11 00:44:15,428 INFO: [debug..][epoch:  0, iter:       2, lr:(2.000e-04,)] [eta: 38 days, 6:34:18, time (data): 28.756 (6.342)] l_pix: 2.4878e-01 \n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python .codebases/HAT/hat/train.py -opt .options/train/hat_2x_div2k.yml --debug"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOQ7Zdc0dz22ZZPtvV+iCnY",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
