{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /Users/gokhankocmarli/Projects/bitirme_tezi/div2k_realesrgan_hat_trainer/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17p7EO3-WUaJ"
      },
      "source": [
        "# Preperation of the Environment & Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUDSkDmLYBjD"
      },
      "source": [
        "**Dowload the original HAT codebase.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5Jkg3AKWDV9"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/XPixelGroup/HAT.git .codebases/HAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ixd68KYJyj"
      },
      "source": [
        "**Install dependencies.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a61va9wJYFSF"
      },
      "outputs": [],
      "source": [
        "%pip install torch torchvision torchaudio\n",
        "%pip install -r .codebases/HAT/requirements.txt\n",
        "%cd .codebases/HAT/\n",
        "!python setup.py develop\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAleu695Y8ws"
      },
      "source": [
        "**Download dataset DIV2K and unzip it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCQ8zcw0ZAM4"
      },
      "outputs": [],
      "source": [
        "!wget -c http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip -P .datasets/DIV2K\n",
        "!unzip .datasets/DIV2K/DIV2K_train_HR.zip -d .datasets/DIV2K\n",
        "!wget -c http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip -P .datasets/DIV2K\n",
        "!unzip .datasets/DIV2K/DIV2K_valid_HR.zip -d .datasets/DIV2K\n",
        "!wget -c http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X2.zip -P .datasets/DIV2K\n",
        "!unzip .datasets/DIV2K/DIV2K_train_LR_bicubic_X2.zip -d .datasets/DIV2K\n",
        "!wget -c http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X2.zip -P .datasets/DIV2K\n",
        "!unzip .datasets/DIV2K/DIV2K_valid_LR_bicubic_X2.zip -d .datasets/DIV2K\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Rename the directories.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mv .datasets/DIV2K/DIV2K_train_HR .datasets/DIV2K/train_hr\n",
        "!mv .datasets/DIV2K/DIV2K_valid_HR .datasets/DIV2K/valid_hr\n",
        "!mv .datasets/DIV2K/DIV2K_train_LR_bicubic/X2 .datasets/DIV2K/train_lr\n",
        "!mv .datasets/DIV2K/DIV2K_valid_LR_bicubic/X2 .datasets/DIV2K/valid_lr\n",
        "!rm -rf .datasets/DIV2K/DIV2K_train_LR_bicubic\n",
        "!rm -rf .datasets/DIV2K/DIV2K_valid_LR_bicubic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvJtcbjsa16f"
      },
      "source": [
        "**Get the mutliscale converter & sub-image maker scripts from Real-ESRGAN codebase.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aiZOsU9bAgZ"
      },
      "outputs": [],
      "source": [
        "!mkdir .scripts\n",
        "!wget https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/scripts/generate_multiscale_DF2K.py -P .scripts\n",
        "!wget https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/scripts/extract_subimages.py -P .scripts\n",
        "!wget https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/scripts/generate_meta_info.py -P .scripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwAl2CYPgo8C"
      },
      "source": [
        "**Generate sub-images.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU1FELDjgoUs"
      },
      "outputs": [],
      "source": [
        "!python .scripts/extract_subimages.py --input .datasets/DIV2K/train_hr --output .datasets/DIV2K/sub_train_hr --crop_size 400 --step 200\n",
        "!python .scripts/extract_subimages.py --input .datasets/DIV2K/valid_hr --output .datasets/DIV2K/sub_valid_hr --crop_size 400 --step 200\n",
        "!python .scripts/extract_subimages.py --input .datasets/DIV2K/train_lr --output .datasets/DIV2K/sub_train_lr --crop_size 200 --step 100\n",
        "!python .scripts/extract_subimages.py --input .datasets/DIV2K/valid_lr --output .datasets/DIV2K/sub_valid_lr --crop_size 200 --step 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the original images.\n",
        "!rm -r .datasets/DIV2K/train_hr\n",
        "!rm -r .datasets/DIV2K/valid_hr\n",
        "!rm -r .datasets/DIV2K/train_lr\n",
        "!rm -r .datasets/DIV2K/valid_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Check the size of the dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check the sizes of the datasets' image count.\n",
        "EQUALITY_CHECK = {\n",
        "    \"sub_train_hr\": \"sub_train_lr\",\n",
        "    \"sub_valid_hr\": \"sub_valid_lr\",\n",
        "}\n",
        "\n",
        "for hr, lr in EQUALITY_CHECK.items():\n",
        "    hr_path = os.path.join(\".datasets/DIV2K\", hr)\n",
        "    lr_path = os.path.join(\".datasets/DIV2K\", lr)\n",
        "    hr_images = os.listdir(hr_path)\n",
        "    lr_images = os.listdir(lr_path)\n",
        "    if len(hr_images) != len(lr_images):\n",
        "        print(f\"[ERROR] Number of images in {hr} and {lr} are not equal.\")\n",
        "    print(f\"[<OK>] Number of images in {hr} and {lr} are equal.\")\n",
        "\n",
        "\n",
        "# Check the total disk usage.\n",
        "!echo \"\\nTotal Disk Usage\"\n",
        "!du -sh .datasets/DIV2K/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Select a Subset of Whole Dataset for Faster Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change these variables to your own dataset location.\n",
        "METADATAS = {\n",
        "    \"train\": {\n",
        "        'root_location': '.datasets/DIV2K/',\n",
        "        'dataset_location': '.datasets/DIV2K/sub_train_hr/',\n",
        "        'meta_data_location': '.metadatas/DIV2K/meta_info_train.txt'\n",
        "    },\n",
        "    \"valid\": {\n",
        "        'root_location': '.datasets/DIV2K/',\n",
        "        'dataset_location': '.datasets/DIV2K/sub_valid_hr/',\n",
        "        'meta_data_location': '.metadatas/DIV2K/meta_info_valid.txt'\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Generate metadata files.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "from os.path import join, relpath\n",
        "from os import makedirs\n",
        "from cv2 import imread\n",
        "from numpy.random import rand\n",
        "\n",
        "# Create a folder for metadata.\n",
        "makedirs('.metadatas/DIV2K/', exist_ok=True)\n",
        "\n",
        "# Select 10% of each dataset for faster training, and place them into metadata.\n",
        "for label, meta_data in METADATAS.items():\n",
        "    with open(meta_data['meta_data_location'], 'w') as meta_info_file:\n",
        "        # Get all image paths.\n",
        "        all_files = sorted(glob(join(meta_data['dataset_location'], '*')))\n",
        "        \n",
        "        # Traverse all images.\n",
        "        for image_path in all_files:\n",
        "\n",
        "            # Select a random number, if it is greater than 0.2, skip this image.\n",
        "            if rand() > 0.50:\n",
        "                continue\n",
        "\n",
        "            # Check the image.\n",
        "            try:\n",
        "                image = imread(image_path)\n",
        "            except (IOError, OSError) as error:\n",
        "                print(f'Read {image_path} error: {error}')\n",
        "                continue\n",
        "            if image is None:\n",
        "                print(f'Img is None: {image_path}')\n",
        "                continue\n",
        "\n",
        "            # Get the relative path.\n",
        "            img_name = relpath(image_path, meta_data['root_location'])\n",
        "            \n",
        "            # Write into metadata.\n",
        "            meta_info_file.write(f'{img_name}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The data length for validation and training sets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the line count of each metadata.\n",
        "for label, meta_data in METADATAS.items():\n",
        "    with open(meta_data['meta_data_location'], 'r') as meta_info_file:\n",
        "        print(f'{meta_data[\"meta_data_location\"]} has {len(meta_info_file.readlines())} lines.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Configuration File for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Copy a new configuration file from the original one.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p .options/train\n",
        "!cp .codebases/HAT/options/train/train_HAT-S_SRx2_from_scratch.yml .options/train/hat_2x_div2k.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the configuration YAML file and change the parameters needed.\n",
        "import yaml\n",
        "\n",
        "with open('.options/train/hat_2x_div2k.yml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "if config['datasets']['train']['name'] != \"DIV2K_Training_\":\n",
        "    config['datasets']['train']['name'] = \"DIV2K_Training\"\n",
        "    config['datasets']['train']['dataroot_gt'] = \".datasets/DIV2K/\"\n",
        "    config['datasets']['train']['dataroot_lq'] = \".datasets/DIV2K/sub_train_lr\" # It needs sub-dir as well.\n",
        "    config['datasets']['train']['meta_info_file'] = \".datasets/DIV2K/meta_info_train.txt\"\n",
        "    config['datasets']['val']['name'] = \"DIV2K_Validation\"\n",
        "    config['datasets']['val']['dataroot_gt'] = \".datasets/DIV2K/\"\n",
        "    config['datasets']['val']['dataroot_lq'] = \".datasets/DIV2K/sub_valid_lr\"  # It needs sub-dir as well.\n",
        "    config['datasets']['val']['meta_info_file'] = \".datasets/DIV2K/meta_info_valid.txt\"\n",
        "\n",
        "    # Remove unused validation.\n",
        "    if 'val_2' in config['datasets']:\n",
        "        del config['datasets']['val_2']\n",
        "    if 'val_3' in config['datasets']:\n",
        "        del config['datasets']['val_3']\n",
        "\n",
        "    # Enable auto-resume has to be done using options file.\n",
        "    config['auto_resume'] = True\n",
        "    config['logger']['print_freq'] = 50\n",
        "    config['logger']['save_checkpoint_freq'] = 1000.0\n",
        "\n",
        "    # Change the total iteration.\n",
        "    config['train']['total_iter'] = 25000\n",
        "\n",
        "    # Save the configuration YAML file.\n",
        "    with open('.options/train/hat_2x_div2k.yml', 'w') as file:\n",
        "        yaml.dump(config, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demo Training (Short Period & w/ Debugging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python .codebases/HAT/hat/train.py -opt .options/train/hat_2x_div2k.yml --debug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python .codebases/HAT/hat/train.py -opt .options/train/hat_2x_div2k.yml --auto_resume"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOQ7Zdc0dz22ZZPtvV+iCnY",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
